---
affiliated_institute:
  en_name: Kyushu University
  name: "\u4E5D\u5DDE\u5927\u5B66"
  url: https://www.kyushu-u.ac.jp
editor_id: github.cbal-brezina
extra_resources: {}
id: de82abed-daa0-4993-b83f-3f151efc08f4
language: en
modified: '2019-11-29T02:01:11.747045Z'
title: LAF - Orthogonality II
title_id: laf-orthogonality-ii
topics: []
translations: {}
version: '1.0'
---

## Intro

In this challenge  we extend the idea of orthogonal projection onto a vector to orthogonal projection onto a vector space (e.g., projection onto a plane in 3D). This allows us to decompose each vector into two mutually orthogonal parts, one in `$W$` and the other in `$W^\perp$`. This decomposition plays an important role in many applications, even where orthogonality has no geometrical meaning. Furthermore, we introduce a procedure that allows us to orthogonalize any basis. Finally, we discuss orthogonal diagonalization, i.e., diagonalizationa with respect to an orthogonal basis. 

## Terminology

- orthogonal projection
- orthogonal component
- the Gram-Schmidt Process
- orthogonal matrix
- orthogonally diagonalizable
 

## Theorems


- orthogonal decomposition theorem
- `${\rm dim}\ W + {\rm dim}\ W^\perp = n$`
- the Gram-Schmidt process
- properties of orthogonal matrices
- the Spectral Theorem



## Key points


- notion of orthogonal projection
- how to apply orthogonal decomposition theorem
- how to find an orthogonal basis using the Gram-Schmidt process
- easy calculation with orthogonal matrices thanks to their properties
- properties of eigenvalues and eigen vectors of real symmetric matrices
- orthogonal diagonalization of symmetric matrices



## Challenge



1. Find the orthogonal projection of `${\bf v}$` onto the subspace `$W$` spanned by the vectors `${\bf u}_i$`. (You may do not need to verify that the vectors `${\bf u}_i$` are orthogonal.)

  a) `${\bf v} = \left[\begin{array}{r} 1 \\ 2 \\3 \end{array}\right], \ \ \ {\bf u}_1 = \left[\begin{array}{r} 1\\1\\1 \end{array}\right], \ \ \ {\bf u}_2 = \left[\begin{array}{r} 1\\-1\\0 \end{array}\right]$`

  b) `${\bf v} = \left[\begin{array}{r} 4\\-2\\-3\\2 \end{array}\right], \ \ \ {\bf u}_1 = \left[\begin{array}{r} 1\\1\\0\\1 \end{array}\right], \ \ \ {\bf u}_2 = \left[\begin{array}{r} 0\\1\\1\\-1 \end{array}\right], \ \ \ {\bf u}_3 = \left[\begin{array}{r} -1\\0\\1\\1 \end{array}\right]$`

2. Find the orthogonal decomposition of `${\bf v}$` with respect to `$W$`.

  a) `${\bf v} = \left[\begin{array}{r} 3 \\ 2 \\-1 \end{array}\right]$`, `$W = \left(\left[\begin{array}{r} 1 \\ 1 \\1 \end{array}\right]\right)$`

  b)  `${\bf v} = \left[\begin{array}{r} 4 \\ -2 \\3 \end{array}\right]$`, `$W = \left(\left[\begin{array}{r} 1 \\ 2 \\1 \end{array}\right], \left[\begin{array}{r} 1 \\-1 \\1 \end{array}\right]\right)$`

3. The given vectors form a basis for a subspace `$W$` of `$\mathbb R^3$` or `$\mathbb R^4$`. Apply the Gram-Schmidt Process to obtain an orthogonal basis for `$W$`.

  a) `${\bf x}_1 = \left[\begin{array}{r} 1 \\ 1 \\0 \end{array}\right], \ \ \ {\bf x}_2 = \left[\begin{array}{r} 3\\4\\2 \end{array}\right]$`

  b) `$ {\bf x}_1 = \left[\begin{array}{r} 1\\2\\-2 \\ 1 \end{array}\right], \ \ \ {\bf x}_2 = \left[\begin{array}{r} 1\\1 \\ 0 \\2 \end{array}\right],  \ \ \ {\bf x}_3 = \left[\begin{array}{r} 1\\8 \\ 1 \\0 \end{array}\right]$`
  
4. Find an orthogonal basis for `$\mathbb R^4$` that contains the vectors `$ \left[\begin{array}{r} 2\\1\\0\\-1 \end{array}\right], \ \ \  \left[\begin{array}{r} 1\\0\\3\\2\end{array}\right].$`

5. Determine whether the given matrix is orthogonal. If it is, find its inverse.
  
  a) `$\left[\begin{array}{rr} 0 & -1 \\ 1&0 \end{array}\right]$`

  b) `$\left[\begin{array}{rrrr} \frac12 & \frac12 & -\frac12&\frac12 \\ \frac12 & \frac12 & \frac12& -\frac12 \\ -\frac12 & \frac12 & \frac12& \frac12 \\ \frac12 & -\frac12 & \frac12&\frac12  \end{array}\right]$`

6. Orthogonally diagonalize  matrix `$A$`, i.e., find an orthogonal matrix `$Q$` and a diagonal matrix `$D$` such that `$Q^TAQ = D$`.

  a) `$ A = \left[\begin{array}{rrr} 1 &0&-1 \\ 0&1&0\\ -1&0&1\end{array}\right]$`
  
  b) `$ A = \left[\begin{array}{rrrr} 1&1&0&0 \\ 1&1&0&0\\ 0&0&1&1\\ 0&0&1&1 \end{array}\right]$`


7. Let `$W$` be a subspace of `$\mathbb R^n$` and `${\bf v}$` a vector in `$\mathbb R^n$`. Suppose that `${\bf w}$` and `${\bf w}'$` are orthogonal vectors with `${\bf w}\in W$` and  `${\bf v} = {\bf w} + {\bf w}'$`. Is it necessarily true that `${\bf w}'$` is in `$W^\perp$`? Either prove that it is true or find a counterexample. 

8. Let `$\{{\bf v}_1,\dots,{\bf v}_n\}$` be an orthogonal basis for `$\mathbb R^n$` and let `$W = {\rm span}({\bf v}_1,\dots,{\bf v}_k)$`. Is it necessarily true that `$W^\perp = {\rm span}({\bf v}_{k+1},\dots, {\bf v}_n)$`?  Either prove that it is true or find a counterexample. 

9.  If `$A$` is an invertible matrix that is orthogonally diagonalizable, show that `$A^{-1}$` is orthogonally diagonalizable. 


## Resources

- Check textbook

- [The geometric view on orthogonal projections by Trefor Bazett](https://youtu.be/2dGXQwYDaqU)

- [Orthogonal Decomposition Theorem Part II by Trefor Bazett](https://youtu.be/0EX--joLoiM)
 
- [Using Gram-Schmidt to orthogonalize a basis by Trefor Bazett](https://youtu.be/LXE9NeaLQsc)

- [Full example: using Gram-Schmidt by Trefor Bazett](https://youtu.be/zti01DiImiQ)

- [Proving that orthogonal projections are a form of minimization by Trefor Bazett](https://youtu.be/xJX5Y016ZEU)

- [Comparison of eigenvalues and eigenvectors for real symmetric, antisymmetric and orthogonal matrices by MIT OpenCourseWare](https://youtu.be/ZTNniGvY5IQ)

- [The geometry behind dot product by 3blue1brown](https://youtu.be/LyGKycYT2v0)

- Explore the Internet

## Tips


- You learn more from trying and failing than from an immediate explanation.

- Find and solve as many exercises as YOU need to become proficient.

- Required minimum must be set, however it is a waste of time for everybody if you do not aim higher.






